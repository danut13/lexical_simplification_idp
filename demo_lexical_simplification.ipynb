{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forced-playback",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from feature_based_solution import execute_demo_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aging-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.textsimplifier import simplify_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impressive-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.simplifier import Simplifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-annual",
   "metadata": {},
   "source": [
    "# TRAINING THE COMPLEX WORD IDENTIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "literary-scholarship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english: 27299 training - 3328 dev\n",
      "Feature based models\n",
      "Number of models: 3\n",
      "Training models\n",
      "Training:  Nearest Neighbors\n",
      "Training:  RBF SVM\n",
      "Training:  Decision Tree\n",
      "Predicting labels\n",
      "Calculating scores\n",
      "Scores for Nearest Neighbors\n",
      "macro-F1: 0.7876\n",
      "Scores for RBF SVM\n",
      "macro-F1: 0.8227\n",
      "Scores for Decision Tree\n",
      "macro-F1: 0.7819\n",
      "Scores for hard voting with all models\n",
      "macro-F1: 0.8243\n"
     ]
    }
   ],
   "source": [
    "model = execute_demo_feature(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-proceeding",
   "metadata": {},
   "source": [
    "# TRAINING THE SIMPLIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interior-airport",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "simplifier = Simplifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spanish-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\\\n",
    "The Longyearbyen cemetery is at the base of a steep hill, just beyond the town limits. If you look up from the cemetery, you can see the gray wooden skeleton of the coal mine that used to burrow into the side of the hill, and if you look to your left you can see the icy fringes of a glacier. Farther down the mountain are a shallow stream, a broad shale plain, and then, half a mile or so across the valley, Longyearbyen itself: a small cluster of red-roofed, brightly painted frame buildings. There are no trees, because Longyearbyen is many miles above the tree line, and from almost anywhere in the valley the cemetery is in plain view. Each grave site is slightly elevated and surrounded by rocks, and there are well-worn pathways among the rows of crosses. A chain-link fence rings the periphery. When I was there in late August, the ground had been warmed by the Arctic summer sun and was soft and spongy, carpeted with orange and red and white lichen. In the last row I found the miners’ graves—seven deaths separated by six days.\n",
    "\n",
    "It is possible to go to almost any cemetery in the world and find a similar cluster of graves from the fall of 1918. Between September and November of that year, as the First World War came to an end, an extraordinarily lethal strain of influenza swept the globe, killing between twenty million and forty million people. More Americans died of the flu over the next few months than were killed during the First World War, the Second World War, the Korean War, and the Vietnam War combined. The Spanish flu, as it came to be known, reached every continent and virtually every country on the map, going wherever ships sailed or cars or trucks or trains travelled, killing so many so quickly that some cities were forced to convert streetcars into hearses, and others buried their dead in mass graves, because they ran out of coffins. Of all those millions of graves, though, the seven in Longyearbyen stand apart. There, less than eight hundred miles from the North Pole, the ground beneath the lichen is hard-frozen permafrost. The bodies of the seven miners may well be intact, cryogenically preserved in the tundra, and, if so, the flu virus they caught on board the Forsete—the deadliest virus that the world has ever known—may still be down there with them.\n",
    "\n",
    "At the beginning of next month, a scientific team led by the Canadian geographer Kirsty Duncan will fly to Longyearbyen and set up a workstation in the church graveyard. The team will map the site, and then scan it with ground-penetrating radar, passing what looks like a small black vacuum cleaner over the tundra to see how deep the bodies are buried. If the radar shows that they are below the active layer of the permafrost—that is, below the layer that thaws each summer—the team will return next fall with enough medical equipment and gear to outfit a small morgue. The site will be prepared with tarpaulins and duckboards. Pavement breakers—electric jackhammers—will be used to break up the tundra, and the chunks of earth will be scooped out with a shovel. As the excavation gets close to the coffins, the diggers will don biohazard spacesuits, and a dome or a tent will be erected over the operation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-wisdom",
   "metadata": {},
   "source": [
    "## SIMPLIFY TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify_text(text, model, simplifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-rebound",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}